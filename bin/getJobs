#!/usr/bin/python3

#
# get all jobs run from filter
# get the session ids for those analyses
# Get the flywheel paths for those sessions sorting by group/project

import argparse
import csv
import flywheel
import fwgearutils
import json
import os
import pyjq
import re
import sys
import yaml

# *** Need to add comments to suss out techdev scans

from collections import OrderedDict
from dateutil import parser

CmdName = os.path.basename(sys.argv[0])
Epoch = parser.parse('2020-01-01')

def progress(count, total, status=''):
    bar_len = 60
    filled_len = int(round(bar_len * count / float(total)))

    percents = round(100.0 * count / float(total), 1)
    bar = '=' * filled_len + '-' * (bar_len - filled_len)

    sys.stderr.write('%s/%s [%s] %s%s ...%s\r' % (i, total, bar, percents, '%', status))
    sys.stderr.flush()  # As suggested by Rom Ruben (see: http://stackoverflow.com/questions/3173320/text-progress-bar-in-the-console/27871113#comment50529068_27871113)

# *** nice to have job duration, cpus,  cpu_sec, where it ran, 

InitialSessionAnalysis = {}

def isSessionReaped(session_id=None, debug=False):
    session = fw.get(session_id)
    for a in session.acquisitions():
        try:
            acquisition_origin_id = a.files[0].origin.id
            if (acquisition_origin_id in UIDs2Labels.keys()):
                if (debug):
                    print("id {} from {}".format(acquisition_origin_id, UIDs2Labels[acquisition_origin_id]), file=sys.stderr)
                return(True)

        except (AttributeError) as e:
            True

    return(False)
    return(False)
    
#
# category types seem to be:
# analysis
# classifier
# converter
# qa
#
def isAnalysisJob(job):
    JobCategory = None

    try:
        JobCategory = job.gear_info.category
    except (AttributeError) as e:
        print("{}: Job {} has no gear_info.category".format(e, job.id), file=sys.stderr)

    return(JobCategory == "analysis")

def getPostEpochAcquisitionIds(job=None, debug=False):
    InputAcquisitionIds = []
    PostEpochAcquisitionIds = {}

    if (job.config['inputs'] == None):
        inputs = []
    elif (type(job.config['inputs']) == dict):
        inputs = [ job.config['inputs'] ]
    else:
        inputs = job.config['inputs']

    for i in inputs:
        InputAcquisitionIds.extend(pyjq.all('..|select(.type == "acquisition")? | .id',i))

    for aid in InputAcquisitionIds:
        try:
            a = fw.get(aid)
        except (flywheel.rest.ApiException) as e:
            print("{}: No such acquisition {} in job {}".format(e, aid, job.id), file=sys.stderr)
            continue

        a = a.reload()

        try:
            AcquisitionDate = parser.parse(str(a.files[0].info['AcquisitionDate']))
            if (AcquisitionDate > Epoch):
                PostEpochAcquisitionIds[aid] = AcquisitionDate
            else:
                if (debug):
                    print("AcqusitionDate for {} of Job {} is {} before the Epoch {}".format(
                        aid,
                        job.id,
                        AcquisitionDate,
                        Epoch), file=sys.stderr)

        except (KeyError) as e:
            print("{}: Acquisition {} of job {} has no AcquisitionDate".format(e, aid, job.id), file=sys.stderr)

    return(PostEpochAcquisitionIds)

def acquisitionId2SessionId(AcquisitionId):
    Acquisition = fw.get(AcquisitionId)
    return(Acquisition.parents.session)

def isJobInitialSessionAnalysis(JobId=None, SessionId=None, debug=False):
    
    session_id = SessionId
    session = fw.get(SessionId)

    if (session_id in InitialSessionAnalysis.keys()):
        if (JobId == InitialSessionAnalysis[session_id]):
            if (debug):
                print("{} already in InitialSessionAnalysis".format(session_id), file=sys.stderr)

            return(True)

    analyses = sorted(session.analyses, key=lambda a: a.created)
    if (len(analyses)):
        if (debug):
            stuff = []
            for a in analyses:
                stuff.append({'id': a.job.id, 'date': str(a.created)})
            print("session({}).analyses = {}".format(session_id,stuff), file=sys.stderr)

        if (JobId == analyses[0].job.id):
            InitialSessionAnalysis[session_id] = analyses[0].job.id
            if (debug):
                print("match {} = {}".format(JobId, analyses[0].job.id), file=sys.stderr)

            return(True)

        if (debug):
            print("session {} has initial analysis {} not job {}".format(session_id, analyses[0].job.id,JobId), file=sys.stderr)

    return(False)
     
def getBillableSessionIds(job=None, debug=False):
    BillableSessionIds = {}

    if (isAnalysisJob(job)):
        if (debug):
            print("Job {} is analysis".format(job.id), file=sys.stderr)

        PostEpochAcquisitionIds = getPostEpochAcquisitionIds(job=job, debug=debug)
        if (len(PostEpochAcquisitionIds.keys())):
            if (debug):
                print("PostEpochAcquisitionIds = {}".format(', '.join(PostEpochAcquisitionIds)), file=sys.stderr)
        else:
            if (debug):
                print("job {} has no PostEpochAcquisitionIds".format(job.id), file=sys.stderr)

        for AcquisitionId,AcquisitionDate in PostEpochAcquisitionIds.items():
            SessionId = acquisitionId2SessionId(AcquisitionId)
            if (isJobInitialSessionAnalysis(JobId=job.id, SessionId=SessionId, debug=debug)):
                BillableSessionIds[SessionId] = {
                    'JobId': job.id,
                    'AcquisitionId': AcquisitionId,
                    'AcquisitionDate': str(PostEpochAcquisitionIds[AcquisitionId]),
                    'JobRunDate': job.created,
                }

    else:
        if (debug):
            print("{} not analysis job".format(job.id), file=sys.stderr)

    return(BillableSessionIds)


def initRow(
	job_date=None,
	job_id=None,
        job_elapsed_time_ms=None,
        job_runtime_ms=None,
	gear_id=None,
	gear_name=None,
	gear_version=None,
	gear_category=None,
	job_origin_id=None,
	job_origin_type=None,
	job_state=None,
        job_cpus=None,
        gcp_cpus=None,
        gcp_compute_percent=None,
	initial_analysis=None,
        group=None,
        project=None,
        subject=None,
        session=None,
        session_id=None,
	acquisition_name=None,
	acquisition_id=None,
        gcp_compute_cost=None,
    ):

    row = OrderedDict([
	( 'job_date', job_date ),
	( 'job_id', job_id ),
	( 'gear_id', gear_id ),
	( 'gear_name', gear_name ),
	( 'gear_version', gear_version ),
	( 'gear_category', gear_category ),
	( 'job_origin_id', job_origin_id ),
	( 'job_origin_type', job_origin_type ),
	( 'job_state', job_state ),
        ( 'job_elapsed_time_ms', job_elapsed_time_ms ), 
        ( 'job_runtime_ms', job_runtime_ms ), 
        ( 'job_cpus', job_cpus), 
        ( 'gcp_cpus', gcp_cpus), 
        ( 'gcp_compute_percent', gcp_compute_percent),
	( 'initial_analysis', initial_analysis ),
        ( 'group', group ),
        ( 'project', project ),
        ( 'subject', subject ),
        ( 'session', session ),
        ( 'session_id', session_id ),
	( 'acquisition_name', acquisition_name ),
	( 'acquisition_id', acquisition_id ),
        ( 'gcp_compute_cost', gcp_compute_cost),
    ])
    return(row)

CondorYamlFile = "/home/holder/flywheel/etc/condor.yml"

# jq '.[][][]|select(.engineMatch.whitelist and .engineMatch.whitelist["gear-name"] and (.engineMatch.whitelist["gear-name"][]| match("^fmriprep-phases$"))) | .cloud.machineType' /tmp/json
# 

with open(CondorYamlFile) as f:
    CondorYaml = yaml.load(f, Loader=yaml.FullLoader)

def getJobMachineType(GearName, Tags):
    MachineType = 'n1-standard-1'

    MachineTypes = pyjq.all('.[][][]|select(.engineMatch.whitelist and .engineMatch.whitelist["gear-name"] and (.engineMatch.whitelist["gear-name"][]| match("^{}$"))) | .cloud.machineType'.format(GearName), CondorYaml)
    if (len(MachineTypes) > 0):
        MachineType = MachineTypes[0]
    else:
        # *** Should do something about tags here
        True

    return(MachineType)

#
# *** Need to handle tags correctly
#
def getJobCpus(job,detail):
    GearName = detail['gear_info']['name']

    if (re.search('hpc',GearName)):
        cpus = 0
    elif (job.profile.executor and job.profile.executor.cpu_cores > 0):
        cpus = job.profile.executor.cpu_cores 
    else:
        MachineType = getJobMachineType(GearName, job.tags)

        m = re.match('^(?P<class>[^-]+)-(?P<type>[^-]+)-(?P<cores>[^-]+)$',MachineType)
        if (m):
            cpus = int(m.group('cores'))
        else:
            cpus = 0

    return(cpus)

ap = argparse.ArgumentParser()

ap.add_argument('-a', '--augment-cpu-count', action='store_true', help='do extra work to lookup cpu count')
ap.add_argument('-b', '--bar-length', action='store', default=40, type=int, help='bar length')
ap.add_argument('-c', '--gcp-compute-cost', action='store', type=float, help='Total GCP Compute cost for Flywheel ')
ap.add_argument('-d', '--debug', action='store_true', help='debug')

ap.add_argument('-e', '--exhaustive', action='store_false', default=True, help='exhaustive session flag')
ap.add_argument('-E', '--epoch', type=str, help='Epoch date YYYY-MM-DD')
ap.add_argument('-i', '--initial-analysis', action='store_true', help='check for initial analysis flag')
ap.add_argument('-l', '--limit', action='store', default=1000000, type=int, help='Flywheel cost allocation csv file')
ap.add_argument('-p', '--progressbar', action='store_true', help='show progress bar')
ap.add_argument('-v', '--verbose', action='store_true', help='verbose')
ap.add_argument('filter', nargs='*', type=str, default=None, help='arguments to fw.find()')

args = ap.parse_args()

if (args.epoch):
    Epoch = parser.parse(str(args.epoch))

UIDs2Labels = {}

fw = fwgearutils.getFW(args, Root=True)

reapers = fw.get_all_devices()
for r in reapers:
    UIDs2Labels[r.id] = r.name

Gears = {}
gears = fw.get_all_gears()
for g in gears:
    UIDs2Labels[g.id] = g.gear.name
    Gears[g.id] = g

projects = fw.get_all_projects(exhaustive=args.exhaustive, limit=args.limit)
for p in projects:
    UIDs2Labels[p.id] = p.label

subjects = fw.get_all_subjects(exhaustive=args.exhaustive, limit=args.limit)
for s in subjects:
    UIDs2Labels[s.id] = s.label

filter=','.join(args.filter)
#print("filter = filter={}".format(filter), file=sys.stderr)
jobs = []
if (len(args.filter)):
#  jobs = fw.get_all_jobs(limit=args.limit, filter=filter)
  j = fw.jobs.iter_find(*args.filter)
  for i in j:
      jobs.append(i)
else:
  jobs = fw.get_all_jobs(limit=args.limit)

print("Len jobs = ",len(jobs), file=sys.stderr)

l = len(jobs)
i = 0


Sessions = {}

OutputRows = []
ComputeGCPMs = 0.0

for job in jobs:
    job = job.reload()

    if (args.progressbar):
        progress(i, l)
        i += 1

    detail = fw.get_job_detail(job.id)
    if (args.verbose):
        print("job", job, "detail", detail, file=sys.stderr)

    try:
        job_id = job.id
        gear_id = job.gear_id
        job_origin_id = job.origin.id
        job_origin_type = job.origin.type

        if (getattr(detail.parent_info, 'session') and getattr(detail.parent_info.session, 'id')):
            session_id = detail.parent_info.session.id
        else:
            session_id = None

        if (job_origin_id and job_origin_id in UIDs2Labels.keys()):
            job_origin_id = UIDs2Labels[job_origin_id]

        if (getattr(detail.parent_info, 'session') and getattr(detail.parent_info.session, 'id')):
            session_id = detail.parent_info.session.id
        else:
            session_id = None

        if (getattr(detail.parent_info, 'acquisition') and getattr(detail.parent_info.acquisition, 'id')):
            acquisition_id = detail.parent_info.acquisition.id
        else:
            acquisition_id = None
            # print("detail missing acquisition info", detail, file=sys.stderr)

        try:
            cores = job['profile']['executor']['cpu_cores']
        except (AttributeError,TypeError) as e:
            cores = 0

        if(job['profile']['elapsed_time_ms']):
            job_elapsed_time_ms = job['profile']['elapsed_time_ms']
        else:
            job_elapsed_time_ms = 0.0
            
        if (args.augment_cpu_count):
            gcp_cores = getJobCpus(job, detail)
        elif (re.search('hpc', job.gear_info.name)):
            gcp_cores = 0
        else:
            gcp_cores = cores

        if (args.initial_analysis):
            InitialAnalysisCount = (len(getBillableSessionIds(job=job, debug=args.debug)) > 0)

        if (detail.parent_info.session.label):
            session_label = re.sub('\s*$','',re.sub('[\\n\\r]',' ',detail.parent_info.session.label))
        else:
            session_label = ""

        try:
            out = initRow(
                job_date=job.created,
                job_id=job.id,
                gear_id=job.gear_id,
                gear_name=job.gear_info.name,
                gear_version=job.gear_info.version,
                gear_category=job.gear_info.category,
                job_origin_id=job.origin.id,
                job_origin_type=job.origin.type,
                job_state=job.state,
                job_elapsed_time_ms=job_elapsed_time_ms,
                job_runtime_ms=job['profile']['total_time_ms'],
                job_cpus=cores,
                gcp_cpus=gcp_cores,
                initial_analysis=InitialAnalysisCount,
                group=detail.parent_info.group.id,  #ID is one word lowercase, label can be words and mixed case
                project=detail.parent_info.project.label,
                subject=detail.parent_info.subject.label,
                session=session_label,
                session_id=detail.parent_info.session.id,
                acquisition_id=acquisition_id
            )
        except (TypeError) as e:
            print("detail.parent_info.session.label = '{}'".format(detail.parent_info.session.label), file=sys.stderr)

        if (job['profile']['elapsed_time_ms']):
            ComputeGCPMs += float(gcp_cores) * float(job['profile']['elapsed_time_ms'])

        OutputRows.append(out)

    except (AttributeError) as e:
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        print(exc_type, fname, exc_tb.tb_lineno, file=sys.stderr)

print("len(OutputRows) = ",len(OutputRows), file=sys.stderr)

out = initRow()
writer = csv.DictWriter(sys.stdout, out.keys(),lineterminator='\n' )
writer.writeheader()
for out in OutputRows:

    # if all the jobs are in HCP, gcp ms == 0
    if (ComputeGCPMs):
        out['gcp_compute_percent'] = float(out['gcp_cpus']) * float(out['job_elapsed_time_ms']) / ComputeGCPMs
        out['gcp_compute_cost'] = out['gcp_compute_percent'] * float(args.gcp_compute_cost)

    writer.writerow(out)

#    if (args.verbose):
#        print(json.dumps(fwgearutils.sloppyCopy(session, recurse=True), indent=2), file=sys.stderr)



